{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T14:17:13.837432Z",
     "start_time": "2025-02-03T14:16:53.205974Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()"
   ],
   "id": "6e2402200589a7dd",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T14:20:32.190756Z",
     "start_time": "2025-02-03T14:19:11.178418Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")"
   ],
   "id": "c94eded41eebe236",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T07:42:50.705237Z",
     "start_time": "2025-02-04T07:42:50.662033Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
   ],
   "id": "271c42f03fbeac04",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T06:37:20.586372Z",
     "start_time": "2025-02-04T06:37:18.137369Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "chain = llm | output_parser\n",
    "result = chain.invoke(\"Tell me a joke\")\n",
    "print(result)"
   ],
   "id": "81b9f19220c13e30",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the scarecrow win an award? \n",
      "\n",
      "Because he was outstanding in his field!\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "'''from typing import List\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class MobileReview(BaseModel):\n",
    "    phone_model: str = Field(description=\"Name and model of the phone\")\n",
    "    rating: float = Field(description=\"Overall rating out of 5\")\n",
    "    pros: List[str] = Field(description=\"List of positive aspects\")\n",
    "    cons: List[str] = Field(description=\"List of negative aspects\")\n",
    "    summary: str = Field(description=\"Brief summary of the review\")\n",
    "\n",
    "review_text = \"\"\"\n",
    "Just got my hands on the new Galaxy S21 and wow, this thing is slick! The screen is gorgeous,\n",
    "colors pop like crazy. Camera's insane too, especially at night - my Insta game's never been\n",
    "stronger. Battery life's solid, lasts me all day no problem.\n",
    "Not gonna lie though, it's pretty pricey. And what's with ditching the charger? C'mon Samsung.\n",
    "Also, still getting used to the new button layout, keep hitting Bixby by mistake.\n",
    "Overall, I'd say it's a solid 4 out of 5. Great phone, but a few annoying quirks keep it from\n",
    "being perfect. If you're due for an upgrade, definitely worth checking out!\n",
    "\"\"\"\n",
    "\n",
    "structured_llm = llm.with_structured_output(MobileReview)\n",
    "output = structured_llm.invoke(review_text)\n",
    "print(output)\n",
    "print(output.pros)'''"
   ],
   "id": "1705367dac97e71f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me a short joke about {topic}\")\n",
    "chain = prompt | llm | output_parser\n",
    "result = chain.invoke({\"topic\": \"programming\"})\n",
    "print(result)"
   ],
   "id": "809e984ce5b093b4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "Human_Message = \"\"\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant that tells jokes.\"),\n",
    "    HumanMessage(content=\"{Human_Message}\")\n",
    "]\n",
    "response = llm.invoke(messages)\n",
    "print(response)\n",
    "\n",
    "template = ChatPromptTemplate([\n",
    "    (\"system\", \"You are a helpful assistant that tells jokes.\"),\n",
    "    (\"human\", \"Tell me about {topic}\")\n",
    "])\n",
    "chain = template | llm\n",
    "response = chain.invoke({\"topic\": \"programming\"})\n",
    "print(response)"
   ],
   "id": "452a11fdb0d21d97"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T07:53:47.388969Z",
     "start_time": "2025-02-04T07:53:46.834142Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "import os\n",
    "\n",
    "def load_documents(folder_path: str) -> List[Document]:\n",
    "    documents = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        if filename.endswith('.pdf'):\n",
    "            loader = PyPDFLoader(file_path)\n",
    "        elif filename.endswith('.docx'):\n",
    "            loader = Docx2txtLoader(file_path)\n",
    "        else:\n",
    "            print(f\"Unsupported file type: {filename}\")\n",
    "            continue\n",
    "        documents.extend(loader.load())\n",
    "    return documents\n",
    "\n",
    "folder_path = \"/Users/viditparashar/Documents/Papers\"\n",
    "documents = load_documents(folder_path)\n",
    "print(f\"Loaded {len(documents)} documents from the folder.\")\n"
   ],
   "id": "82b3eb96d9bf83ab",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 19 documents from the folder.\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T07:54:22.221506Z",
     "start_time": "2025-02-04T07:54:22.214738Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "splits = text_splitter.split_documents(documents)\n",
    "print(f\"Split the documents into {len(splits)} chunks.\")"
   ],
   "id": "b8261dbd7a9a5434",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split the documents into 112 chunks.\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T07:55:27.420215Z",
     "start_time": "2025-02-04T07:54:31.355534Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "document_embeddings = embedding_function.embed_documents([split.page_content for split in splits])\n",
    "print(document_embeddings[0][:5])\n"
   ],
   "id": "477799bd206a5d2d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j5/n1bxjzvx5g5bjvl_cp_bxhgh0000gn/T/ipykernel_12596/851758213.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "90b8ea82dd07482798248214b7fe3a6b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "50641913a6064fb2b5e593c2cbc1bc80"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5d71d983b7444da4bcbe5f3819e20a35"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bfe38642754f4f3396fb5312c5a1ebc3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "eb1451fe68834409be5ede8cde24aee7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c8c637b8a05d4179a946ddd5276ca4c3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9250f077efda4954a0c65adaf09240f6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "239f3d2c98174e87b9f97829efc5b846"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "23d75bc4306643ca8dc338bd24956c7e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "addd81b29db54283af51c8d0e10b1182"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b252f57766b84414a95cd8d2927edb28"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.059489138424396515, -0.02193422242999077, -0.04421456903219223, -0.008920797146856785, 0.025547431781888008]\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T07:56:02.921738Z",
     "start_time": "2025-02-04T07:56:01.159518Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "collection_name = \"my_collection\"\n",
    "vectorstore = Chroma.from_documents(\n",
    "    collection_name=collection_name,\n",
    "    documents=splits,\n",
    "    embedding=embedding_function,\n",
    "    persist_directory=\"./chroma_db\"\n",
    ")\n",
    "print(\"Vector store created and persisted to './chroma_db'\")\n"
   ],
   "id": "7294200dd4a4c024",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store created and persisted to './chroma_db'\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T07:56:25.094247Z",
     "start_time": "2025-02-04T07:56:18.705877Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query = \"\"\n",
    "search_results = vectorstore.similarity_search(query, k=2)\n",
    "print(f\"\\nTop 2 most relevant chunks for the query: '{query}'\\n\")\n",
    "for i, result in enumerate(search_results, 1):\n",
    "    print(f\"Result {i}:\")\n",
    "    print(f\"Source: {result.metadata.get('source', 'Unknown')}\")\n",
    "    print(f\"Content: {result.page_content}\")\n",
    "    print()"
   ],
   "id": "282665e0f10c4a24",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 2 most relevant chunks for the query: 'LSTM'\n",
      "\n",
      "Result 1:\n",
      "Source: /Users/viditparashar/Documents/Papers/1-s2.0-S1877050922020993-main-2.pdf\n",
      "Content: the previous step, thereby solving the issues of long -term dependencies of RNN where  the RNN give precise \n",
      "predictions on rece nt information but are incapable of predicting data stored in long term memory. Some  of the \n",
      "major applications of LSTM are captioning images, generation of handwriting chatbots answering questions, and \n",
      "various others. [45]\n",
      "\n",
      "Result 2:\n",
      "Source: /Users/viditparashar/Documents/Papers/1-s2.0-S1877050922020993-main-2.pdf\n",
      "Content: the previous step, thereby solving the issues of long -term dependencies of RNN where  the RNN give precise \n",
      "predictions on rece nt information but are incapable of predicting data stored in long term memory. Some  of the \n",
      "major applications of LSTM are captioning images, generation of handwriting chatbots answering questions, and \n",
      "various others. [45] \n",
      " \n",
      " Author name / Procedia Computer Science 00 (2022) 000–000  9 \n",
      " \n",
      " \n",
      " \n",
      "Fig.8: Long Short-Term Memory Algorithm [46] \n",
      " \n",
      "5.  METHODOLOGY \n",
      "The methodology for any research project is the core aspect of obtaining authentic and accurate results. Therefore, \n",
      "the methodology adopted for this research work has been carefully curated and scientifically designed so as to \n",
      "render reliable and wholesome conclusions from the implementation. The methodology used in this research project \n",
      "is majorly described through the following steps. The first step involves the collection of raw data from various open\n",
      "\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T08:02:22.149204Z",
     "start_time": "2025-02-04T08:02:21.952247Z"
    }
   },
   "cell_type": "code",
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "retriever_results = retriever.invoke(query)\n",
    "print(retriever_results)"
   ],
   "id": "fcd7af0d1e668b57",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(id='ec27c7f7-78c0-4978-95e9-ab1a98ee001e', metadata={'page': 7, 'page_label': '254', 'source': '/Users/viditparashar/Documents/Papers/1-s2.0-S1877050922020993-main-2.pdf'}, page_content='the previous step, thereby solving the issues of long -term dependencies of RNN where  the RNN give precise \\npredictions on rece nt information but are incapable of predicting data stored in long term memory. Some  of the \\nmajor applications of LSTM are captioning images, generation of handwriting chatbots answering questions, and \\nvarious others. [45]'), Document(id='438441e4-eeb3-40e2-ad3a-0de307e7f283', metadata={'page': 8, 'page_label': '255', 'source': '/Users/viditparashar/Documents/Papers/1-s2.0-S1877050922020993-main-2.pdf'}, page_content='the previous step, thereby solving the issues of long -term dependencies of RNN where  the RNN give precise \\npredictions on rece nt information but are incapable of predicting data stored in long term memory. Some  of the \\nmajor applications of LSTM are captioning images, generation of handwriting chatbots answering questions, and \\nvarious others. [45] \\n \\n Author name / Procedia Computer Science 00 (2022) 000–000  9 \\n \\n \\n \\nFig.8: Long Short-Term Memory Algorithm [46] \\n \\n5.  METHODOLOGY \\nThe methodology for any research project is the core aspect of obtaining authentic and accurate results. Therefore, \\nthe methodology adopted for this research work has been carefully curated and scientifically designed so as to \\nrender reliable and wholesome conclusions from the implementation. The methodology used in this research project \\nis majorly described through the following steps. The first step involves the collection of raw data from various open')]\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T08:06:40.952193Z",
     "start_time": "2025-02-04T08:06:40.935937Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "Question: {question}\n",
    "Answer: \"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "def docs2str(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | docs2str, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ],
   "id": "b4dbc6b81c19d5a2",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T08:09:10.949189Z",
     "start_time": "2025-02-04T08:08:27.031828Z"
    }
   },
   "cell_type": "code",
   "source": [
    "question = \"\"\n",
    "response = rag_chain.invoke(question)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {response}\")"
   ],
   "id": "b29b767708253b3c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How is LSTM used in stock prediction?\n",
      "Answer: The context does not provide specific details on how Long Short-Term Memory (LSTM) is used in stock prediction. It only mentions that the DL (Deep Learning) algorithm, which includes LSTM, outperforms other algorithms for stock price or time series prediction with extensive accuracy. For detailed methods or applications of LSTM in stock prediction, further information would be required.\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-06T14:51:11.802857Z",
     "start_time": "2025-02-06T14:51:09.022006Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "contextualize_q_system_prompt = \"\"\"\n",
    "Given a chat history and the latest user question\n",
    "which might reference context in the chat history,\n",
    "formulate a standalone question which can be understood\n",
    "without the chat history. Do NOT answer the question,\n",
    "just reformulate it if needed and otherwise return it as is.\n",
    "\"\"\"\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "contextualize_chain = contextualize_q_prompt | llm | StrOutputParser()\n",
    "print(contextualize_chain.invoke({\"input\": \"How to predict stock??\", \"chat_history\": []}))\n"
   ],
   "id": "b5e340a6d910a347",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What methods can be used to predict stock market trends?\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-07T14:08:21.422809Z",
     "start_time": "2025-02-07T14:08:21.410084Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI assistant. Use the following context to answer the user's question.\"),\n",
    "    (\"system\", \"Context: {context}\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
   ],
   "id": "603084ff0503ffcd",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-07T14:10:05.433643Z",
     "start_time": "2025-02-07T14:09:54.951125Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "chat_history = []\n",
    "question1 = \"Is LSTM used in it?\"\n",
    "answer1 = rag_chain.invoke({\"input\": question1, \"chat_history\": chat_history})['answer']\n",
    "chat_history.extend([\n",
    "    HumanMessage(content=question1),\n",
    "    AIMessage(content=answer1)\n",
    "])\n",
    "\n",
    "print(f\"Human: {question1}\")\n",
    "print(f\"AI: {answer1}\\n\")\n",
    "\n",
    "question2 = \"if not lstm then what is used?\"\n",
    "answer2 = rag_chain.invoke({\"input\": question2, \"chat_history\": chat_history})['answer']\n",
    "chat_history.extend([\n",
    "    HumanMessage(content=question2),\n",
    "    AIMessage(content=answer2)\n",
    "])\n",
    "\n",
    "print(f\"Human: {question2}\")\n",
    "print(f\"AI: {answer2}\")"
   ],
   "id": "3f23f1c060b94057",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Is LSTM used in it?\n",
      "AI: Yes, Long Short-Term Memory (LSTM) is used in this context. Some of its major applications mentioned include captioning images, generating handwriting, and enabling chatbots to answer questions.\n",
      "\n",
      "Human: if not lstm then what is used?\n",
      "AI: The context provided only mentions the use of the Long Short-Term Memory (LSTM) algorithm. If not LSTM, other types of recurrent neural networks (RNNs) or different machine learning algorithms could potentially be used, but the specific alternative is not mentioned in the provided context.\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T06:04:07.420955Z",
     "start_time": "2025-02-10T06:04:07.414283Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Multi-User\n",
    "\n",
    "import sqlite3\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "\n",
    "DB_NAME = \"rag_app.db\"\n",
    "\n",
    "def get_db_connection():\n",
    "    conn = sqlite3.connect(DB_NAME)\n",
    "    conn.row_factory = sqlite3.Row\n",
    "    return conn\n",
    "\n",
    "def create_application_logs():\n",
    "    conn = get_db_connection()\n",
    "    conn.execute(''' CREATE TABLE IF NOT EXISTS application_logs( id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    session_id TEXT,\n",
    "    user_query TEXT,\n",
    "    gpt_response TEXT,\n",
    "    model TEXT,\n",
    "    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP)''')\n",
    "    conn.close()\n",
    "    \n",
    "def insert_application_logs(session_id, user_query, gpt_response, model):\n",
    "    conn = get_db_connection()\n",
    "    conn.execute('INSERT INTO application_logs(session_id, user_query, gpt_response, model) VALUES(?, ?, ?, ?)', (session_id, user_query, gpt_response, model))\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    \n",
    "def get_chat_history(session_id):\n",
    "    conn = get_db_connection()\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('SELECT user_query, gpt_response FROM application_logs WHERE session_id = ? ORDER BY created_at', (session_id,))\n",
    "    messages = []\n",
    "    for row in cursor.fetchall():\n",
    "        messages.extend([\n",
    "            {\"role\": \"human\",\"content\": row[\"user_query\"]},\n",
    "            {\"role\": \"human\",\"content\": row[\"gpt_response\"]}\n",
    "        ])\n",
    "    conn.close()\n",
    "    \n",
    "    return messages\n",
    "\n",
    "create_application_logs()"
   ],
   "id": "510722eecbabe858",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T07:39:21.955271Z",
     "start_time": "2025-02-12T07:39:15.389062Z"
    }
   },
   "cell_type": "code",
   "source": [
    "session_id = str(uuid.uuid4())\n",
    "question = \"what is this paper about\"\n",
    "chat_history = get_chat_history(session_id)\n",
    "answer = rag_chain.invoke({\"input\": question, \"chat_history\": chat_history})['answer']\n",
    "insert_application_logs(session_id, question, answer, \"gpt-4o-mini\")\n",
    "print(f\"Human: {question}\")\n",
    "print(f\"AI: {answer}\")"
   ],
   "id": "f17faed346f9cf86",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: what is this paper about\n",
      "AI: This paper is about the stock market, specifically discussing the various factors that can cause drastic oscillations in stock prices. These factors can include politics, brand reputation, and the current global scenario. The paper emphasizes that while understanding these factors is essential, they are not sufficient for developing a method for accurate prediction of stock market trends due to the ever-changing global conditions.\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T07:41:49.821863Z",
     "start_time": "2025-02-12T07:41:42.781923Z"
    }
   },
   "cell_type": "code",
   "source": [
    "question2 = \"how can i use it?\"\n",
    "chat_history = get_chat_history(session_id)\n",
    "answer = rag_chain.invoke({\"input\": question2, \"chat_history\": chat_history})['answer']\n",
    "insert_application_logs(session_id, question2, answer, \"gpt-4o-mini\")\n",
    "print(f\"Human: {question2}\")\n",
    "print(f\"AI: {answer}\")"
   ],
   "id": "647b6e01791fdebf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: how can i use it?\n",
      "AI: This paper can be used to gain a deeper understanding of the factors that influence stock market trends. If you're an investor, you can use this knowledge to make more informed decisions about your investments. If you're a researcher or a student, this paper can serve as a reference for studies on stock market trends and predictions. Furthermore, the paper discusses the use of machine learning techniques for stock market prediction, which could be of interest if you're exploring technological solutions for investment strategies.\n"
     ]
    }
   ],
   "execution_count": 42
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
